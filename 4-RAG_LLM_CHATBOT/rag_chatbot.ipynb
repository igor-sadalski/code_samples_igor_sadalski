{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6750c8",
   "metadata": {},
   "source": [
    "use this line to instal depenencies if it doesnt work try to build the virtual env from the reqirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7219411b-fae6-4c2a-b170-796bc30ed073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from getpass import getpass\n",
    "from urllib.request import urlopen\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from langchain.vectorstores import ElasticsearchStore\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain_core.documents.base import Document \n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from huggingface_hub import notebook_login\n",
    "from elasticsearch.exceptions import ConnectionTimeout\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b184b3a5-0cc8-43f9-b15d-f5ccf48f574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_index_name = \"gemma-rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49427546-7b37-48f4-a6fe-395736ea2d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/elastic/elasticsearch-labs/main/datasets/workplace-documents.json\"\n",
    "\n",
    "response = urlopen(url)\n",
    "\n",
    "workplace_docs = json.loads(response.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf0104-8b31-4b39-ad21-b372fd1fa0db",
   "metadata": {},
   "source": [
    "### Preprocessing the document and splitting into fixed size chunks\n",
    "fix size is easier to debug if something goes bad, i selected a simple text with minimal amount of footers/headers but still needed to clear it from excessive carrige return/new line characters. Decided to leave the quotation marks in the text, as well as some gibberish at the begining end (it is fraction of the total dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d8f78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3917,\n",
       " [Document(page_content='\\ufeffThe Project Gutenberg eBook of A Room with a View This ebook is for the use of anyone anywhere in t'),\n",
       "  Document(page_content='he United States and most other parts of the world at no cost and with almost no restrictions whatso'),\n",
       "  Document(page_content='ever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License in'),\n",
       "  Document(page_content='cluded with this ebook or online at www.gutenberg.org. If you are not located in the United States, '),\n",
       "  Document(page_content='you will have to check the laws of the country where you are located before using this eBook. Title:')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "target_url = \"https://www.gutenberg.org/cache/epub/2641/pg2641.txt\"\n",
    "\n",
    "target_response = urlopen(target_url)\n",
    "\n",
    "content = target_response.read().decode(\"utf-8\")\n",
    "\n",
    "cleaned_text = re.sub(r'[\\n\\r]+', ' ', content)\n",
    "cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "chunk = 100\n",
    "chunked_text = [cleaned_text[i:i+chunk] for i in range(0, len(cleaned_text)-chunk, chunk)]\n",
    "docs = [Document(page_content = chunk) for chunk in chunked_text]\n",
    "len(docs), docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264bc1b-23b1-4547-a7f0-670944c3e605",
   "metadata": {},
   "source": [
    "### Indexing using ElasticSearch, due to some error problems on their side tryign multiple times\n",
    "\n",
    "State of the art, easier to scale-up/down (just rent more compute units from the elastisearch) and monitor durin deployment (they have nice dashboard to monitor the progress and various jobs status). For the retrieval model I used ELSER (Elastic Learned Sparse EncodeR) with the newest version 2. Compared to say BERT is better for indexing large amounts of data (like books we are deadlin with)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468a495b",
   "metadata": {},
   "source": [
    "### Cant generate embedding for the whol sequence since I run out of credits....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb1db78e-e40a-4a5c-9d15-75ee2a1d0994",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "max_attempts = 10\n",
    "for attempt in range(max_attempts):\n",
    "    try:\n",
    "        embedding_space = ElasticsearchStore.from_documents(\n",
    "            docs[:2000],\n",
    "            es_cloud_id=ELASTIC_CLOUD_ID,\n",
    "            es_api_key=ELASTIC_API_KEY,\n",
    "            index_name=elastic_index_name,\n",
    "            timeout = 30,\n",
    "            strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(\n",
    "                model_id=\".elser_model_2\"\n",
    "            ),\n",
    "        )\n",
    "        break\n",
    "    except ConnectionTimeout:  \n",
    "        if attempt < max_attempts - 1:\n",
    "            logger.warning(f\"Connection timed out. Retrying ({attempt + 1}/{max_attempts})...\")\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            logger.error(\"Connection failed after several attempts. Please try again later.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dc04685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The scene is laid in Florence.” “What fun, Cecil! Read away. Come, Mr. Emerson, sit down after all yThe scene is laid in Florence.” “What fun, Cecil! Read away. Come, Mr. Emerson, sit down after all yThe scene is laid in Florence.” “What fun, Cecil! Read away. Come, Mr. Emerson, sit down after all ynkle of church bells. The garden of Windy Corners was deserted except for a red book, which lay sunnnkle of church bells. The garden of Windy Corners was deserted except for a red book, which lay sunn'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'what city is the book taking place?'\n",
    "embedding_space.as_retriever(search_kwargs={\"k\": 5}).invoke(question)\n",
    "\n",
    "docs = embedding_space.as_retriever(search_kwargs={\"k\": 5}).invoke(question)\n",
    "seen = set()\n",
    "for doc in docs:\n",
    "    if doc.page_content not in seen:\n",
    "        seen.add(doc.page_content)\n",
    "    \n",
    "context = ''.join([doc.page_content for doc in docs ])\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b1ead9-c442-40e9-ba81-d4d286ea878b",
   "metadata": {},
   "source": [
    "### Hugging Face login\n",
    "using Hf is nice because we can quikcly experiment with differnt models for our applicaton, its also SOTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2f651e4-e760-4b59-a8a3-57c58dfc229f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbf3e9a875843b2a0711628cb636092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7454f551-71a9-4310-bb2a-3fe0e683daab",
   "metadata": {},
   "source": [
    "### Initialize the tokenizer and the model (`google/gemma-2b-it`)\n",
    "\n",
    "Lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Since these are pretty new and hot right now I chose them over variation of different old and tested methods like T5, BART, XLNet Albert, RoBERT with text-generation variation implemtation in hugging face. For the parameter tunning, I set the temperature to 0.7 experimentaly it seemed to me to give a bit better results on the dataset. Since it runs localy on my PC weights must be top 8GB in RAM which limits possible models. ON smaller model inference time is also quicker so its simpler to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "623e74fb-5707-44f7-9dd8-d9499f7ab61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bec66595b34020aa485ab464a38366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=pipe,\n",
    "    model_kwargs={\"temperature\": 0.7},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e96d42b",
   "metadata": {},
   "source": [
    "### Geerating the answer text based on the RAG output\n",
    "use the augmented prompt with the text-generation selected model, set long max_length in case user would be interested in the summarization of long passages or descriptions often found in books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff1e894d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe book is taking place in Florence.<eos>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f'Answer the question in the following quotation marks: \"{question}\" information provided in the following context surounded by the quotation marks: \"{context}\"'\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids, max_length=200)\n",
    "output_ids = outputs[:, input_ids.shape[-1]:]\n",
    "output_text = tokenizer.decode(output_ids[0])\n",
    "output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8e66ef",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "For metric I could use supervised metrics (e.g. I would manualy craft 20 questions regardin the text with my answer and they dry to detect keyword in chatboot solution to see if it found them, or even read through them and qualitatively assume responses) and more unsupervised techniques e.g. ensembling couple of models, then prompting them with the same question and then clustering their answers together (assuming that consensus is right) and benchamr against this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
